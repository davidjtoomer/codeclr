{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "from codeclr import DenseGraph\n",
    "from codeclr.cass import CassConfig\n",
    "from codeclr.model import ContrastiveLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration / Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = CassConfig(\n",
    "    annot_mode=2,\n",
    "    compound_mode=1,\n",
    "    gfun_mode=1,\n",
    "    gvar_mode=3,\n",
    "    fsig_mode=1)\n",
    "AUGMENTATIONS = ['identity', 'node_drop', 'node_mask', 'subtree_mask']\n",
    "EPOCH = 0\n",
    "DATA_DIR = os.path.join(\n",
    "    'data',\n",
    "    'preprocessed',\n",
    "    'Project_CodeNet_C++1000',\n",
    "    CONFIG.tag\n",
    ")\n",
    "VOCAB_FILE = os.path.join(DATA_DIR, 'vocab.pt')\n",
    "VOCAB = torch.load(VOCAB_FILE)\n",
    "NUM_ANALYSIS_DIRS = 10\n",
    "\n",
    "directories = [d for d in os.listdir(DATA_DIR) if d.startswith('p')]\n",
    "random.shuffle(directories)\n",
    "ANALYSIS_DIRS = directories[:NUM_ANALYSIS_DIRS]\n",
    "print(ANALYSIS_DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_CACHE = {}\n",
    "def get_encoder(augment_1, augment_2, mask_frac: float = 0.25):\n",
    "    if (augment_1, augment_2) in ENCODER_CACHE:\n",
    "        return ENCODER_CACHE[(augment_1, augment_2)]\n",
    "\n",
    "    parameter_tag = parameter_tag = f'augment_1={augment_1}_augment_2={augment_2}_mask_frac={mask_frac}_batch_size=64_lr=0.001_{CONFIG.tag}'\n",
    "    CHECKPOINT_FILE = os.path.join(\n",
    "        'logs',\n",
    "        parameter_tag,\n",
    "        f'checkpoint_{EPOCH}.pt'\n",
    "    )\n",
    "    gcn_layers = [128, 128, 64, 32]\n",
    "    model = ContrastiveLearner(gcn_layers, len(VOCAB) + 1)\n",
    "    model.load_state_dict(torch.load(CHECKPOINT_FILE)['model_state_dict'])\n",
    "    encoder = model.encoder\n",
    "    ENCODER_CACHE[(augment_1, augment_2)] = encoder\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_GRAPH_EMBEDDINGS = {}\n",
    "for augment_1 in AUGMENTATIONS:\n",
    "    for augment_2 in AUGMENTATIONS:\n",
    "        encoder = get_encoder(augment_1, augment_2)\n",
    "        for problem_name in ANALYSIS_DIRS:\n",
    "            problem_dir = os.path.join(DATA_DIR, problem_name)\n",
    "            graphs = [DenseGraph(**torch.load(os.path.join(problem_dir, graph_file))) for graph_file in os.listdir(problem_dir)]\n",
    "            graph_embeddings = encoder(graphs)\n",
    "            ANALYSIS_GRAPH_EMBEDDINGS[(augment_1, augment_2, problem_name)] = graph_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L_1$ Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_distance(anchor_embeddings, auxiliary_embeddings, num_bins: int = 100):\n",
    "    anchor_distances = -torchmetrics.functional.pairwise_cosine_similarity(anchor_embeddings).flatten().detach().numpy()\n",
    "    anchor_auxiliary_distances = -torchmetrics.functional.pairwise_cosine_similarity(anchor_embeddings, auxiliary_embeddings).flatten().detach().numpy()\n",
    "    \n",
    "    bins = np.linspace(-1, 1, num_bins)\n",
    "    anchor_counts, _ = np.histogram(anchor_distances, bins=bins, density=True)\n",
    "    anchor_auxiliary_counts, _ = np.histogram(anchor_auxiliary_distances, bins=bins, density=True)\n",
    "\n",
    "    distance = np.mean(np.abs(anchor_counts - anchor_auxiliary_counts)) / 2\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_distance_stats():\n",
    "    average_l1_distances = {}\n",
    "    max_l1_distances = {}\n",
    "    for augment_1 in AUGMENTATIONS:\n",
    "        for augment_2 in AUGMENTATIONS:\n",
    "            l1_distances = []\n",
    "            for anchor_problem_name in ANALYSIS_DIRS:\n",
    "                for auxiliary_problem_name in ANALYSIS_DIRS:\n",
    "                    if anchor_problem_name == auxiliary_problem_name:\n",
    "                        continue\n",
    "                    anchor_embeddings = ANALYSIS_GRAPH_EMBEDDINGS[(augment_1, augment_2, anchor_problem_name)]\n",
    "                    auxiliary_embeddings = ANALYSIS_GRAPH_EMBEDDINGS[(augment_1, augment_2, auxiliary_problem_name)]\n",
    "                    l1_distances.append(l1_distance(anchor_embeddings, auxiliary_embeddings))\n",
    "            average_l1_distances[(augment_1, augment_2)] = np.mean(l1_distances)\n",
    "            max_l1_distances[(augment_1, augment_2)] = np.max(l1_distances)\n",
    "    return average_l1_distances, max_l1_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_l1_distances, max_l1_distances = l1_distance_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(5, 8))\n",
    "ax1, ax2 = axes\n",
    "\n",
    "average_data_array = np.array([[average_l1_distances[(augment_1, augment_2)] for augment_2 in AUGMENTATIONS] for augment_1 in AUGMENTATIONS])\n",
    "max_data_array = np.array([[max_l1_distances[(augment_1, augment_2)] for augment_2 in AUGMENTATIONS] for augment_1 in AUGMENTATIONS])\n",
    "\n",
    "im = ax1.imshow(average_data_array, cmap='Blues')\n",
    "ax1.set_xticks(np.arange(len(AUGMENTATIONS)), labels=AUGMENTATIONS)\n",
    "ax1.set_yticks(np.arange(len(AUGMENTATIONS)), labels=AUGMENTATIONS)\n",
    "plt.setp(ax1.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "cbar = ax1.figure.colorbar(im, ax=ax1)\n",
    "\n",
    "im = ax2.imshow(max_data_array, cmap='Purples')\n",
    "ax2.set_xticks(np.arange(len(AUGMENTATIONS)), labels=AUGMENTATIONS)\n",
    "ax2.set_yticks(np.arange(len(AUGMENTATIONS)), labels=AUGMENTATIONS)\n",
    "plt.setp(ax2.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "cbar = ax2.figure.colorbar(im, ax=ax2)\n",
    "\n",
    "for i in range(len(AUGMENTATIONS)):\n",
    "    for j in range(len(AUGMENTATIONS)):\n",
    "        text = ax1.text(j, i, round(average_data_array[i, j], 2), ha='center', va='center')\n",
    "        text = ax2.text(j, i, round(max_data_array[i, j], 2), ha='center', va='center')\n",
    "\n",
    "ax1.set_title(r'Average $L_1$ Distance')\n",
    "ax2.set_title(r'Max $L_1$ Distance')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('l1_distance.svg', format='svg', dpi=1200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('codeclr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6184ce5a0a46a57614b39becda2c240a7c387f3b7bcd8924edaa2c64f3413ce6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
